{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import nltk\n",
    "import datasketch\n",
    "import tqdm\n",
    "import pytest\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient = pd.read_csv(\"db_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>state_corrected</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>dob_date</th>\n",
       "      <th>age</th>\n",
       "      <th>age_inferred</th>\n",
       "      <th>test_year</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>phone_format</th>\n",
       "      <th>phone_geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221958</td>\n",
       "      <td>matisse</td>\n",
       "      <td>clarke</td>\n",
       "      <td>13</td>\n",
       "      <td>rene street</td>\n",
       "      <td>westella</td>\n",
       "      <td>ellenbrook</td>\n",
       "      <td>2527</td>\n",
       "      <td>wa</td>\n",
       "      <td>wa</td>\n",
       "      <td>19710708.0</td>\n",
       "      <td>1971-07-08</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>08 86018809</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>771155</td>\n",
       "      <td>joshua</td>\n",
       "      <td>elrick</td>\n",
       "      <td>23</td>\n",
       "      <td>andrea place</td>\n",
       "      <td>foxdown</td>\n",
       "      <td>east preston</td>\n",
       "      <td>2074</td>\n",
       "      <td>nsw</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19120921.0</td>\n",
       "      <td>1912-09-21</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>02 97793152</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231932</td>\n",
       "      <td>alice</td>\n",
       "      <td>conboy</td>\n",
       "      <td>35</td>\n",
       "      <td>mountain circuit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2305</td>\n",
       "      <td>nsw</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19810905.0</td>\n",
       "      <td>1981-09-05</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>02 20403934</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>465838</td>\n",
       "      <td>sienna</td>\n",
       "      <td>craswell</td>\n",
       "      <td>39</td>\n",
       "      <td>cumberlegeicrescent</td>\n",
       "      <td>jodane</td>\n",
       "      <td>henty</td>\n",
       "      <td>3620</td>\n",
       "      <td>wa</td>\n",
       "      <td>wa</td>\n",
       "      <td>19840809.0</td>\n",
       "      <td>1984-08-09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>02 62832318</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>359178</td>\n",
       "      <td>joshua</td>\n",
       "      <td>bastiaans</td>\n",
       "      <td>144</td>\n",
       "      <td>lowrie street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>campbell town</td>\n",
       "      <td>4051</td>\n",
       "      <td>nsw</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19340430.0</td>\n",
       "      <td>1934-04-30</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>03 69359594</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  patient_id given_name    surname  street_number  \\\n",
       "0           0      221958    matisse     clarke             13   \n",
       "1           1      771155     joshua     elrick             23   \n",
       "2           2      231932      alice     conboy             35   \n",
       "3           3      465838     sienna   craswell             39   \n",
       "4           4      359178     joshua  bastiaans            144   \n",
       "\n",
       "             address_1 address_2         suburb postcode state  \\\n",
       "0          rene street  westella     ellenbrook     2527    wa   \n",
       "1         andrea place   foxdown   east preston     2074   nsw   \n",
       "2     mountain circuit       NaN       prospect     2305   nsw   \n",
       "3  cumberlegeicrescent    jodane          henty     3620    wa   \n",
       "4        lowrie street       NaN  campbell town     4051   nsw   \n",
       "\n",
       "  state_corrected  date_of_birth    dob_date   age  age_inferred  test_year  \\\n",
       "0              wa     19710708.0  1971-07-08  32.0          32.0     2003.0   \n",
       "1             nsw     19120921.0  1912-09-21  34.0          34.0     1946.0   \n",
       "2             nsw     19810905.0  1981-09-05  22.0          22.0     2003.0   \n",
       "3              wa     19840809.0  1984-08-09  30.0          30.0     2014.0   \n",
       "4             nsw     19340430.0  1934-04-30  31.0          31.0     1965.0   \n",
       "\n",
       "  phone_number  phone_format  phone_geo  \n",
       "0  08 86018809          True        8.0  \n",
       "1  02 97793152          True        2.0  \n",
       "2  02 20403934          True        2.0  \n",
       "3  02 62832318          True        2.0  \n",
       "4  03 69359594          True        3.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a fait pour corriger les états, on peut directement utiliser un fuzzy matching sur les chaînes de caractères regroupant toutes les informations pertinentes de chaque row. Cependant, cette opération est en O(n^2).\n",
    "\n",
    "C'est ce qu'on essaie ici, en supposant qu'une même personne peut avoir deux numéros de téléphone (donc on ne l'inclue pas dans la recherche de doublons), mais que les nom, prénom et adresse de quelqu'un identifient cette personne. On va donc inclure le nom, les deux adresses ainsi que le code postal dans ce processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define better_scorer so that levenshtein package is not needed\n",
    "def better_scorer(s1: str, s2: str) -> int:\n",
    "    fuzz_ratio = fuzz.ratio(s1, s2)\n",
    "    fuzz_ratio -= (s2.find(s1)*5)\n",
    "    return fuzz_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste une première fois sur une partie du dataset (les 1000 premières lignes) afin d'estimer le temps que prendra cette fonction pour s'exécuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration:  18.309269905090332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "819"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient[\"name_pc_info\"] = df_patient.apply(\n",
    "        lambda row: \" \".join(str(info) for info in row[[\"given_name\", \"surname\", \"postcode\", \"street_number\", \"address_1\", \"address_2\"]]).replace(\" nan \", \" \"), axis=1)\n",
    "\n",
    "start = time.time()\n",
    "df_patient_name_pc_deduped = process.dedupe(df_patient.name_pc_info.iloc[:1000], scorer = better_scorer)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Duration: \", end-start)\n",
    "\n",
    "len([str(element) for element in df_patient_name_pc_deduped])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc pour 1000 lignes, environ 18 secondes d'exécution et on trouve 819 lignes si on enlève les doublons. Sachant qu'il y a encore 19000 lignes à parcourir, on risque de trouver beaucoup de doublons correspondant à ces 819 lignes restantes, elles vont donc encore diminuer.\n",
    "\n",
    "18 secondes sachant que la complexité est en O(n^2), on peut estimer que pour le dataset complet, on prendra 400 fois plus de temps ( (20.000^2) / (1000^2) = 400), soit plus de deux heures sur l'ordinateur sur lequel je travaille (qui est loin d'être le plus performant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*J'ai écrit le bout de code suivant la première fois que j'ai lancé process.dedupe sur l'entièreté des données, afin de ne pas avoir à le faire à chaque fois.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# df_patient_name_pc_deduped = process.dedupe(df_patient.name_pc_info, scorer = better_scorer)\n",
    "# to_save = [str(element) for element in df_patient_name_pc_deduped]\n",
    "    \n",
    "# pickle.dump(to_save, open(\"deduped.pkl\", \"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_patients = pickle.load(open(\"deduped.pkl\", \"rb\"))\n",
    "\n",
    "len(deduped_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe ici une réduction conséquente des données, de presque 50% (il reste 10283/19597 = 52% des données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient[\"is_original\"] = df_patient.apply(lambda row: row.name_pc_info in deduped_patients, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_patient))\n",
    "df_patient_no_duplicates = df_patient.drop_duplicates(subset=\"name_pc_info\", inplace=False)\n",
    "\n",
    "len(df_patient_no_duplicates[df_patient_no_duplicates.is_original])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fonction detect_duplicates aura donc pour arguments le dataframe de données à dédupliquer, ainsi que la liste des features qu'on utilisera pour repérer et supprimer les doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df_patient: pd.DataFrame,\n",
    "                     interesting_features: list = \n",
    "                      [\"given_name\", \"surname\", \"postcode\", \"street_number\", \"address_1\", \"address_2\"]) -> pd.DataFrame:\n",
    "    \n",
    "    df_patient[\"all_info\"] = df_patient.apply(\n",
    "        lambda row: \" \".join(str(info) for info in row[interesting_features]).replace(\" nan \", \" \"), axis=1)\n",
    "    \n",
    "    f = process.dedupe(df_patient.all_info, scorer = better_scorer)\n",
    "\n",
    "    result_df = df_patient.drop_duplicates(subset=\"all_info\")\n",
    "\n",
    "    result_df[\"original\"] = result_df.apply(lambda row: row.all_info in f, axis=1)\n",
    "    result_df = result_df[result_df.original].drop(columns=[\"all_info\", \"original\"])\n",
    "    \n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.20836901664734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test = detect_duplicates(df_patient.iloc[:1000])\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819, 25)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve bien ici les 18 secondes et 819 clés uniques trouvées initialement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To uncomment only right before running for the last time before pushing\n",
    "\n",
    "# start = time.time()\n",
    "# df_patient_no_duplicates = detect_duplicates(df_patient)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme prend plus de 8000 secondes, soit près de 2h30 pour 20000 lignes sur mon ordinateur. Cela reste acceptable pour ces données, mais on peut imaginer une base de données beaucoup plus grande (selon https://www.data.gouv.fr/fr/reuses/statistiques-de-tests-de-depistage-du-coronavirus-covid-19-en-france-par-region-et-par-departement/ plus de 9 millions de tests ont été réalisés à ce jour en France). Dans ce cas, une complexité en O(n^2) n'est pas envisageable.\n",
    "\n",
    "Au lieu de cela, essayons d'utiliser le LSH, qui regroupe des éléments similaires dans une même classe avec une grande probabilité. Une fois les groupes formés, nous pourrons reprendre le fuzzy matching sur chaque sous-groupe. Le LSH réalise un hash où plus deux éléments (ici des chaînes de caractères) sont similaires, plus leur probabilité de collision est forte.\n",
    "\n",
    "Cependant, nous risquons de passer à côté de certains doublons, puisqu'il arrive que des chaînes proches ne se retrouvent pas dans le même groupe avec le LSH. C'est pourquoi dans les arguments donnés en entrées de LSH, on donne plus de poids aux faux négatifs qu'aux faux positifs (qui seront de toute façon filtrés par le fuzzy matching plus tard): c'est l'argument 'weights' de la fonction detect_duplicates_lsh. On ne peut cependant pas mettre un poids trop fort, en risquant de se retrouver avec deux groupes avec la moitié des échantillons chacun par exemple, ce qui en effet minimise le risque de mettre deux éléments se ressemblant dans différents groupes, mais ne nous aide pas pour la phase de fuzzy matching en temps réduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates_lsh(df_patient: pd.DataFrame,\n",
    "                      interesting_features: list = [\"given_name\", \"surname\", \"postcode\", \"state_corrected\",\n",
    "                                              \"street_number\", \"address_1\", \"address_2\", \"suburb\"],\n",
    "                      jaccard_threshold: float=0.4, \n",
    "                      num_perm: int=128, \n",
    "                      weights: tuple=(0.1,0.9), \n",
    "                      nb_ngrams: int=3,\n",
    "                      fuzzy_threshold: int=65) -> pd.DataFrame:\n",
    "    \n",
    "    # Create columns grouping all information of interest to deduplicate rows\n",
    "    df_patient[\"all_info\"] = df_patient.apply(\n",
    "        lambda row: \" \".join(str(info) for info in row[interesting_features]).replace(\" nan \", \" \"). replace(\" nat \", \" \"), axis=1)\n",
    "\n",
    "    # Locality sensitive hashing\n",
    "    lsh = datasketch.MinHashLSH(threshold=jaccard_threshold, num_perm=num_perm, weights=weights)\n",
    "\n",
    "    minhashes = {}\n",
    "    for c, i in enumerate(tqdm.tqdm(df_patient[\"all_info\"])):\n",
    "        minhash = datasketch.MinHash(num_perm=num_perm)\n",
    "        for d in nltk.ngrams(i, nb_ngrams):\n",
    "            minhash.update(\"\".join(d).encode('utf-8'))\n",
    "        lsh.insert(c, minhash)\n",
    "        minhashes[c] = minhash\n",
    "        \n",
    "    df_patient[\"group_size\"] = 0\n",
    "    for i in tqdm.tqdm(range(len(df_patient))):\n",
    "        df_patient.at[i, \"group_size\"] = len(lsh.query(minhashes[i]))\n",
    "        \n",
    "    # Find duplicates in groups found\n",
    "    df_patient[\"processed\"] = False \n",
    "    df_patient[\"original\"] = False \n",
    "    df_patient[\"group\"] = 0\n",
    "    group = 0\n",
    "\n",
    "    # Loop through all rows of dataframe\n",
    "    for i in tqdm.tqdm(range(len(df_patient))):\n",
    "        current_element = df_patient.iloc[i]\n",
    "\n",
    "        # If the current element is not already part of a group\n",
    "        if not current_element.processed:\n",
    "            group +=1\n",
    "            df_patient.at[i, \"processed\"] = True\n",
    "            df_patient.at[i, \"original\"] = True\n",
    "            df_patient.at[i, \"group\"] = group\n",
    "\n",
    "            # Retrieve all elements colliding with the current one\n",
    "            values_to_compare = df_patient.iloc[lsh.query(minhashes[i])]\n",
    "            values_to_compare = values_to_compare[values_to_compare.processed == False].all_info\n",
    "            # print(len(values_to_compare))\n",
    "\n",
    "            # Extract distance of current element to every other colliding element\n",
    "            result_comparison = process.extract(current_element.all_info, values_to_compare, scorer = better_scorer)\n",
    "            \n",
    "            string_match = [comp[0] for comp in result_comparison]\n",
    "            list_match = [comp[0] for comp in result_comparison if comp[1]>fuzzy_threshold]\n",
    "\n",
    "            # Mark matches as processed\n",
    "            for m in list_match:\n",
    "                df_patient.at[df_patient.loc[df_patient.all_info==m].index, \"processed\"] = True\n",
    "                df_patient.at[df_patient.loc[df_patient.all_info==m].index, \"group\"] = True\n",
    "                \n",
    "    return(df_patient)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19597/19597 [01:43<00:00, 189.38it/s]\n",
      "100%|██████████| 19597/19597 [00:05<00:00, 3675.76it/s]\n",
      "100%|██████████| 19597/19597 [01:30<00:00, 217.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.531742811203\n",
      "0.9193754146042762\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "detect_duplicates_lsh(df_patient, fuzzy_threshold = 75, interesting_features = [\"given_name\", \"surname\", \"postcode\",\"address_1\", \"address_2\", \"suburb\", \"street_number\"])\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(len(df_patient[df_patient.original])/len(df_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient.to_csv(\"australia_covid_no_dup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode de calcul a l'avantage de réduire grandement le temps de calcul par rapport à la méthode précédente, ce qui nous permet de gérer une base de données potentiellement de plus en plus grande (si par exemple, il y a chaque jour des mises à jour des données avec de nouveaux patients et de nouveaux tests) sans GPU.\n",
    "\n",
    "Cependant, le nombre de doublons trouvés n'est pas comparable à la quantité de doublons trouvés par fuzzywuzzy uniquement. Il semblerait que la probabilité de collision ne soit pas assez grande pour se contenter de ne regarder que dans le groupe de chaque valeur. De plus, il y a beaucoup plus de paramètres à fixer qui peuvent modifier l'output (en diminuant *fuzzy_treshold*, on va par exemple avoir beaucoup plus de duplicates; par exemple, si l'on augmente le nombre de features utilisées, il faut descendre *fuzzy_threshold* en conséquence puisque la similarité peut baisser avec la longueur de la chaîne de caractères). Cela étant, cette fonction reste intéressante pour des bases de données plus conséquentes, où le fuzzy matching est trop coûteux en temps, afin de repérer une partie au moins des doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  given_name surname   address_1 address_2 street_number postcode\n",
      "0       jane     doe  willstreet                       1     1234\n",
      "2       john     doe  abbey road                       2      456\n"
     ]
    }
   ],
   "source": [
    "# Tests pytest\n",
    "\n",
    "TEST_INPUT = pd.DataFrame({\"given_name\": [\"jane\", \"jane\", \"john\", \"jon\"],\n",
    "                               \"surname\": [\"doe\", \"dooe\", \"doe\", \"doe\"],\n",
    "                               \"address_1\": [\"willstreet\", \"will street\", \"abbey road\", \"abbey\"],\n",
    "                               \"address_2\": [\"\", \"\", \"\", \"road\"],\n",
    "                               \"street_number\": [\"1\", \"1\", \"2\", \"\"],\n",
    "                               \"postcode\": [\"1234\", \"12\", \"456\", \"456\"]})\n",
    "\n",
    "EXPECTED_RESULT = pd.DataFrame({\"given_name\": [\"jane\", \"john\"],\n",
    "                               \"surname\": [\"doe\", \"doe\"],\n",
    "                               \"address_1\": [\"willstreet\", \"abbey road\"],\n",
    "                               \"address_2\": [\"\", \"\"],\n",
    "                               \"street_number\": [\"1\", \"2\"],\n",
    "                               \"postcode\": [\"1234\", \"456\"]})\n",
    "\n",
    "def test_detect_duplicates() -> None:\n",
    "    result_to_test = detect_duplicates(TEST_INPUT)\n",
    "    print(result_to_test)\n",
    "    \n",
    "    for row in result_to_test:\n",
    "        assert (row in EXPECTED_RESULT)\n",
    "        \n",
    "    assert TEST_INPUT.shape[0]>=result_to_test.shape[0]\n",
    "    \n",
    "test_detect_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Il serait intéressant, lorsque plusieurs lignes sont désignées comme doublons, de prendre pour chaque colonne la valeur la plus présente dans le groupe de doublons. Avec fuzzywuzzy, c'est la valeur la plus longues qui est sélectionnée, alors que \"jaane\" devrait être \"jane\" par exemple. Cela pourrait aussi permettre de remplir les données manquantes.\n",
    "\n",
    "- Quand on cherche les doublons dans `detect_duplicates`, il serait intéressant d'y inclure df_pcr dans la liste des arguments; ainsi, on pourrait sélectionner, dans un groupe de doublons, celui dont le patient_id est inclus dans df_pcr, afin de maximiser les données analysables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
